"""
Script Ä‘Æ¡n giáº£n Ä‘á»ƒ cháº¡y inference trÃªn áº£nh/video má»›i
Sá»­ dá»¥ng model Ä‘Ã£ train sáºµn tá»« SavedModels

CÃ¡ch sá»­ dá»¥ng:
    python inference_simple.py --image path/to/image.jpg
    python inference_simple.py --image path/to/image.jpg --output output.png
"""

import torch
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import cv2
import os
import sys
import argparse
import matplotlib.pyplot as plt

# ThÃªm Ä‘Æ°á»ng dáº«n project vÃ o sys.path
BASE_PATH = os.path.dirname(os.path.abspath(__file__))
sys.path.append(BASE_PATH)

from PoseEstimation.ModelArchs import ModelGenerator
from PoseEstimation.Core import Inference
import DataSets.Utils.Config as cfg
import DataSets.Utils.Transforms as transform_utils
import DataSets.Utils.Visualisation as vis

# Image size máº·c Ä‘á»‹nh
IMAGE_SIZE = 256


def preprocess_image(image_path, center=None, scale=None):
    """
    Pre-process áº£nh Ä‘á»ƒ Ä‘Æ°a vÃ o model
    
    Args:
        image_path: ÄÆ°á»ng dáº«n Ä‘áº¿n áº£nh
        center: Tá»a Ä‘á»™ center (náº¿u None, sáº½ dÃ¹ng center cá»§a áº£nh)
        scale: Scale factor (náº¿u None, sáº½ tÃ­nh tá»« áº£nh)
    
    Returns:
        processed_image: áº¢nh Ä‘Ã£ pre-process (tensor)
        center: Tá»a Ä‘á»™ center Ä‘Ã£ dÃ¹ng
        scale: Scale factor Ä‘Ã£ dÃ¹ng
        original_image: áº¢nh gá»‘c (numpy array)
    """
    # Load áº£nh
    image = Image.open(image_path).convert('RGB')
    original_image = np.array(image)
    
    # TÃ­nh center vÃ  scale náº¿u khÃ´ng Ä‘Æ°á»£c cung cáº¥p
    if center is None:
        # Center cá»§a áº£nh
        h, w = original_image.shape[:2]
        center = np.array([w / 2, h / 2])
    
    if scale is None:
        # Scale dá»±a trÃªn kÃ­ch thÆ°á»›c áº£nh
        h, w = original_image.shape[:2]
        scale = max(h, w) / 200.0  # scale in relation to 200px
    
    # Transform áº£nh
    rotation = 0
    trans = transform_utils.get_affine_transform(
        center, scale, rotation, IMAGE_SIZE
    )
    
    # Warp áº£nh
    processed_image = cv2.warpAffine(
        original_image, trans, (IMAGE_SIZE, IMAGE_SIZE), flags=cv2.INTER_LINEAR
    )
    
    # Convert to tensor
    processed_image = transforms.ToTensor()(processed_image)
    
    return processed_image, center, scale, original_image


def run_inference(image_path, output_path=None, use_bbox_model=False):
    """
    Cháº¡y inference trÃªn áº£nh
    
    Args:
        image_path: ÄÆ°á»ng dáº«n Ä‘áº¿n áº£nh
        output_path: ÄÆ°á»ng dáº«n Ä‘á»ƒ lÆ°u káº¿t quáº£ (náº¿u None, sáº½ hiá»ƒn thá»‹)
        use_bbox_model: CÃ³ dÃ¹ng bbox model Ä‘á»ƒ detect infant khÃ´ng (tÃ¹y chá»n)
    """
    print("=" * 70)
    print("CHáº Y INFERENCE TRÃŠN áº¢NH")
    print("=" * 70)
    print(f"áº¢nh input: {image_path}")
    
    # Kiá»ƒm tra file tá»“n táº¡i
    if not os.path.exists(image_path):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file {image_path}")
        return
    
    # Setup device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")
    
    # Load models
    print("\n1. Äang load models...")
    
    try:
        # Táº¡o model trá»±c tiáº¿p mÃ  khÃ´ng cáº§n dataset loader
        # (VÃ¬ chÃºng ta chá»‰ cáº§n model Ä‘á»ƒ inference, khÃ´ng cáº§n dataset)
        pose2D_model = ModelGenerator.load2DPoseEstimationModel(device)
        lifting_model = ModelGenerator.get3DLiftingNetwork(device)
        print("   âœ… Models Ä‘Ã£ Ä‘Æ°á»£c táº¡o")
    except Exception as e:
        print(f"   âŒ Lá»—i khi táº¡o models: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Load pretrained weights
    print("\n2. Äang load pretrained weights...")
    model_dir = os.path.join(BASE_PATH, "SavedModels", "SavedModels")
    
    # ÄÆ°á»ng dáº«n Ä‘áº¿n cÃ¡c model
    pose2d_path = os.path.join(model_dir, "MINI_RGBD_2D", "model.tar")
    lifting_path = os.path.join(model_dir, "MINI_RGBD_FineTune", "model.tar")
    
    # Kiá»ƒm tra file model tá»“n táº¡i
    if not os.path.exists(pose2d_path):
        print(f"   âŒ KhÃ´ng tÃ¬m tháº¥y model: {pose2d_path}")
        print(f"   ğŸ’¡ Äáº£m báº£o Ä‘Ã£ download SavedModels tá»« OneDrive")
        print(f"   ğŸ’¡ Link: https://liveuclac-my.sharepoint.com/:u:/g/personal/rmhisje_ucl_ac_uk/EcQr9vyPlSBPmdJSazAIDP0BJ7ydxgrjSmYpeoho1v5efQ?e=4MTIH7")
        return
    
    if not os.path.exists(lifting_path):
        print(f"   âŒ KhÃ´ng tÃ¬m tháº¥y model: {lifting_path}")
        print(f"   ğŸ’¡ Äáº£m báº£o Ä‘Ã£ download SavedModels tá»« OneDrive")
        print(f"   ğŸ’¡ Link: https://liveuclac-my.sharepoint.com/:u:/g/personal/rmhisje_ucl_ac_uk/EcQr9vyPlSBPmdJSazAIDP0BJ7ydxgrjSmYpeoho1v5efQ?e=4MTIH7")
        return
    
    try:
        # Load 2D pose model
        checkpoint = torch.load(pose2d_path, map_location=device)
        pose2D_model.load_state_dict(checkpoint["model_state_dict"])
        pose2D_model.eval()
        print("   âœ… 2D Pose Model Ä‘Ã£ Ä‘Æ°á»£c load")
        
        # Load 3D lifting model
        checkpoint = torch.load(lifting_path, map_location=device)
        lifting_model.load_state_dict(checkpoint["model_state_dict"])
        lifting_model.eval()
        print("   âœ… 3D Lifting Model Ä‘Ã£ Ä‘Æ°á»£c load")
    except Exception as e:
        print(f"   âŒ Lá»—i khi load models: {e}")
        return
    
    # Optional: Load bbox model náº¿u cáº§n
    bbox_model = None
    if use_bbox_model:
        print("\n3. Äang load bbox model...")
        try:
            from FasterRCNN.BoundingBoxModel import BoundingBoxModel
            bbox_path = os.path.join(model_dir, "MINI_RGBD_Bbox", "model.tar")
            if os.path.exists(bbox_path):
                bbox_model = BoundingBoxModel(device, bbox_path)
                print("   âœ… Bbox Model Ä‘Ã£ Ä‘Æ°á»£c load")
            else:
                print("   âš ï¸  KhÃ´ng tÃ¬m tháº¥y bbox model, sáº½ dÃ¹ng center/scale tá»« áº£nh")
        except Exception as e:
            print(f"   âš ï¸  Lá»—i khi load bbox model: {e}")
            print("   âš ï¸  Sáº½ dÃ¹ng center/scale tá»« áº£nh")
    
    # Pre-process áº£nh
    print("\n4. Äang pre-process áº£nh...")
    try:
        # Náº¿u cÃ³ bbox model, dÃ¹ng nÃ³ Ä‘á»ƒ detect infant
        if bbox_model is not None:
            image_pil = Image.open(image_path).convert('RGB')
            scale, center = bbox_model.getCentreAndScale(image_pil)
            print(f"   âœ… ÄÃ£ detect infant vá»›i bbox model")
        else:
            center = None
            scale = None
            print(f"   âœ… Äang pre-process áº£nh (dÃ¹ng center/scale tá»« áº£nh)")
        
        processed_image, center, scale, original_image = preprocess_image(
            image_path, center, scale
        )
        print(f"   âœ… áº¢nh Ä‘Ã£ Ä‘Æ°á»£c pre-process")
        print(f"   Center: {center}, Scale: {scale:.2f}")
    except Exception as e:
        print(f"   âŒ Lá»—i khi pre-process áº£nh: {e}")
        return
    
    # Cháº¡y inference
    print("\n5. Äang cháº¡y inference...")
    try:
        # Chuáº©n bá»‹ input
        input_tensor = processed_image.unsqueeze(0).to(device)
        
        # Inference 2D pose
        with torch.no_grad():
            pose2d_output = pose2D_model(input_tensor)
            pose2d_preds = pose2d_output.detach().cpu().numpy()
        
        # Post-process 2D predictions
        center_array = np.array([[center[1], center[0]]])  # [x, y] format
        scale_array = np.array([scale])
        pred_coords_2d = Inference.postProcessPredictions(
            pose2d_preds, center_array, scale_array, 64
        )
        print("   âœ… 2D pose Ä‘Ã£ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n")
        
        # Inference 3D lifting
        num_joints = cfg.MPII["numJoints"]
        pose2d_flat = torch.tensor(pred_coords_2d).view(-1, num_joints * 2).to(device)
        
        with torch.no_grad():
            pose3d_output = lifting_model(pose2d_flat)
            pose3d_preds = pose3d_output.detach().cpu().view(-1, num_joints, 3).numpy()
        
        print("   âœ… 3D pose Ä‘Ã£ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n")
    except Exception as e:
        print(f"   âŒ Lá»—i khi cháº¡y inference: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Visualize káº¿t quáº£
    print("\n6. Äang visualize káº¿t quáº£...")
    try:
        fig = plt.figure(figsize=(15, 5))
        
        # Config
        connected_joints = cfg.MPII["connectedJoints"]
        joint_colours = cfg.MPII["jointColours"]
        
        # Plot 1: Input image
        ax1 = plt.subplot(1, 3, 1)
        ax1.set_title("Input Image", fontsize=12)
        vis.plotImage(ax1, original_image)
        
        # Plot 2: 2D Pose
        ax2 = plt.subplot(1, 3, 2)
        ax2.set_title("2D Pose Prediction", fontsize=12)
        vis.plotImage(ax2, original_image)
        vis.plot2DJoints(
            ax2, pred_coords_2d[0], connected_joints, joint_colours, None
        )
        
        # Plot 3: 3D Pose
        ax3 = plt.subplot(1, 3, 3, projection='3d')
        ax3.set_title("3D Pose Prediction", fontsize=12)
        vis.plot3DJoints(ax3, pose3d_preds[0], connected_joints, joint_colours)
        
        plt.tight_layout()
        
        # LÆ°u hoáº·c hiá»ƒn thá»‹
        if output_path:
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            print(f"   âœ… Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {output_path}")
        else:
            # Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
            output_dir = os.path.join(BASE_PATH, "Images", "InferenceOutput")
            os.makedirs(output_dir, exist_ok=True)
            
            # TÃªn file output
            input_name = os.path.splitext(os.path.basename(image_path))[0]
            output_path = os.path.join(output_dir, f"{input_name}_result.png")
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            print(f"   âœ… Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {output_path}")
        
        plt.close()
        
    except Exception as e:
        print(f"   âŒ Lá»—i khi visualize: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("\n" + "=" * 70)
    print("âœ… HOÃ€N THÃ€NH!")
    print("=" * 70)


def main():
    parser = argparse.ArgumentParser(description='Cháº¡y inference trÃªn áº£nh/video')
    parser.add_argument('--image', type=str, required=True,
                        help='ÄÆ°á»ng dáº«n Ä‘áº¿n áº£nh input')
    parser.add_argument('--output', type=str, default=None,
                        help='ÄÆ°á»ng dáº«n Ä‘á»ƒ lÆ°u káº¿t quáº£ (tÃ¹y chá»n)')
    parser.add_argument('--use-bbox', action='store_true',
                        help='Sá»­ dá»¥ng bbox model Ä‘á»ƒ detect infant (tÃ¹y chá»n)')
    
    args = parser.parse_args()
    
    run_inference(args.image, args.output, args.use_bbox)


if __name__ == "__main__":
    main()

